{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xUj1kKzIANNT"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZQXsB6t67Yj",
        "outputId": "85d3ed08-9126-446d-8c13-d79afbce625a"
      },
      "outputs": [],
      "source": [
        "!mkdir sentences\n",
        "%cd /content/sentences\n",
        "!tar -xvf /content/drive/MyDrive/IAM_dataset/sentences.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B8JARTid8VUH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW\n",
        "from functorch import make_functional, make_functional_with_buffers, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6hdC_hP0ihSO"
      },
      "outputs": [],
      "source": [
        "class IAM_fewshot_dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 image_dir,\n",
        "                 meta_filename,\n",
        "                 processor,\n",
        "                 max_target_length=128,\n",
        "                 episode_num=600,\n",
        "                 shot=5,):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.episode_num = episode_num\n",
        "        self.shot = shot\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "        with open(meta_filename, 'r') as json_file:\n",
        "            meta_data = json.load(json_file)\n",
        "\n",
        "        for i in range(len(meta_data)):\n",
        "            sample = meta_data[i]\n",
        "            dir = os.path.join(image_dir, sample['image_dir'])\n",
        "            if not os.path.exists(dir):\n",
        "                print(dir, os.path.exists(dir))\n",
        "                raise Exception\n",
        "\n",
        "        self._writer_id_to_ind = {}\n",
        "        writer_ind = 0\n",
        "        for sample in meta_data:\n",
        "            if sample['writer_id'] not in self._writer_id_to_ind:\n",
        "                self._writer_id_to_ind[sample['writer_id']] = writer_ind\n",
        "                writer_ind += 1\n",
        "\n",
        "        self._ind_to_writer_id = {value: key for key, value in self._writer_id_to_ind.items()}\n",
        "\n",
        "        self.writer_samples = [[] for ind in self._ind_to_writer_id]\n",
        "        for sample in meta_data:\n",
        "            writer_id = sample['writer_id']\n",
        "            writer_ind = self._writer_id_to_ind[writer_id]\n",
        "            self.writer_samples[writer_ind].append(sample)\n",
        "\n",
        "        self.writer_num = len(self.writer_samples)\n",
        "\n",
        "    def __len__(self,):\n",
        "        return self.episode_num\n",
        "\n",
        "    def get_encoding(self, sample):\n",
        "        # get file name + text\n",
        "        file_name = os.path.join(self.image_dir, sample['image_dir'])\n",
        "        text = ' '.join(sample['transcription'])\n",
        "\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(text,\n",
        "                                          padding=\"max_length\",\n",
        "                                          max_length=self.max_target_length).input_ids\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get writer\n",
        "        while True:\n",
        "            writer_ind = np.random.randint(0, self.writer_num)\n",
        "            samples = self.writer_samples[writer_ind]\n",
        "            if len(samples) > self.shot:\n",
        "                break\n",
        "\n",
        "        random.shuffle(samples)\n",
        "        supports = samples[:self.shot]\n",
        "        query = samples[self.shot]\n",
        "\n",
        "        supports = [self.get_encoding(sample) for sample in supports]\n",
        "        query = self.get_encoding(query)\n",
        "\n",
        "        pixel_values = []\n",
        "        labels = []\n",
        "        for batch in supports:\n",
        "            pixel_values.append(batch['pixel_values'])\n",
        "            labels.append(batch['labels'])\n",
        "        pixel_values = torch.stack(pixel_values, 0)\n",
        "        labels = torch.stack(labels, 0)\n",
        "        supports = {'pixel_values': pixel_values, \"labels\": labels}\n",
        "        return supports, query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KAAOkYn8c66V"
      },
      "outputs": [],
      "source": [
        "class IAM_global_dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 image_dir,\n",
        "                 meta_filename,\n",
        "                 processor,\n",
        "                 max_target_length=128):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "        with open(meta_filename, 'r') as json_file:\n",
        "            self.meta_data = json.load(json_file)\n",
        "\n",
        "        for i in range(len(self.meta_data)):\n",
        "            sample = self.meta_data[i]\n",
        "            dir = os.path.join(image_dir, sample['image_dir'])\n",
        "            if not os.path.exists(dir):\n",
        "                print(dir, os.path.exists(dir))\n",
        "                raise Exception\n",
        "\n",
        "    def __len__(self,):\n",
        "        return len(self.meta_data)\n",
        "\n",
        "    def get_encoding(self, sample):\n",
        "        # get file name + text\n",
        "        file_name = os.path.join(self.image_dir, sample['image_dir'])\n",
        "        text = ' '.join(sample['transcription'])\n",
        "\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(text,\n",
        "                                          padding=\"max_length\",\n",
        "                                          max_length=self.max_target_length).input_ids\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.meta_data[idx]\n",
        "        sample = self.get_encoding(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHZdp8k38lDr",
        "outputId": "01ce3b8c-8619-4acd-8dfc-124b3e74f9a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
        "\n",
        "global_train_dataset = IAM_global_dataset('/content/sentences', '/content/drive/MyDrive/IAM_dataset/meta_train_data.json', processor=processor)\n",
        "\n",
        "fewshot_train_dataset = IAM_fewshot_dataset('/content/sentences', '/content/drive/MyDrive/IAM_dataset/meta_train_data.json', processor=processor, episode_num=2000)\n",
        "fewshot_test_dataset = IAM_fewshot_dataset('/content/sentences', '/content/drive/MyDrive/IAM_dataset/meta_test_data.json', processor=processor, episode_num=100)\n",
        "fewshot_val_dataset = IAM_fewshot_dataset('/content/sentences', '/content/drive/MyDrive/IAM_dataset/meta_val_data.json', processor=processor, episode_num=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rkLfeY3D8hQL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "global_train_dataloader = DataLoader(global_train_dataset, batch_size=8)\n",
        "\n",
        "fewshot_train_dataloader = DataLoader(fewshot_train_dataset, batch_size=1)\n",
        "fewshot_test_dataloader = DataLoader(fewshot_test_dataset, batch_size=1)\n",
        "fewshot_val_dataloader = DataLoader(fewshot_val_dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktgpL8wE9wC3",
        "outputId": "ed822f45-a5e1-473f-bbda-a7f2357bf39e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): DeiTModel(\n",
              "    (embeddings): DeiTEmbeddings(\n",
              "      (patch_embeddings): DeiTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): DeiTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DeiTLayer(\n",
              "          (attention): DeiTAttention(\n",
              "            (attention): DeiTSelfAttention(\n",
              "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): DeiTSelfOutput(\n",
              "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DeiTIntermediate(\n",
              "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DeiTOutput(\n",
              "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): DeiTPooler(\n",
              "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TrOCRForCausalLM(\n",
              "    (model): TrOCRDecoderWrapper(\n",
              "      (decoder): TrOCRDecoder(\n",
              "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
              "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
              "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x TrOCRDecoderLayer(\n",
              "            (self_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (activation_fn): ReLU()\n",
              "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-printed\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y5lRejjsABeR"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXHLWzpAUGAT",
        "outputId": "21d35268-7638-44d9-a0d4-ad7a2b37e01b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sd = model.state_dict()\n",
        "for k in sd:\n",
        "    sd[k] = torch.randn_like(sd[k])\n",
        "model.load_state_dict(sd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFpy7aGWAEXt",
        "outputId": "a5529f73-58b4-44fd-eb29-dcbdc778c212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-c81d87c6f9c2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  cer_metric = load_metric(\"cer\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/cer/cer.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ImqX8lKxAG2H"
      },
      "outputs": [],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DV_Wn8ZqoIrA"
      },
      "outputs": [],
      "source": [
        "def compute_confidence_interval(data):\n",
        "    \"\"\"\n",
        "    Compute 95% confidence interval\n",
        "    :param data: An array of mean accuracy (or mAP) across a number of sampled episodes.\n",
        "    :return: the 95% confidence interval for this data.\n",
        "    \"\"\"\n",
        "    a = 1.0 * np.array(data)\n",
        "    m = np.mean(a)\n",
        "    std = np.std(a)\n",
        "    pm = 1.96 * (std / np.sqrt(len(a)))\n",
        "    return m, pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cPHSw-2tgsGr"
      },
      "outputs": [],
      "source": [
        "def fewshot_testing(model, fewshot_dataloader):\n",
        "    model.eval()\n",
        "    valid_cer = []\n",
        "    inner_iter_num = 5\n",
        "    init_state_dict = model.state_dict()\n",
        "\n",
        "    for supports, query in tqdm(fewshot_dataloader):\n",
        "        model.load_state_dict(init_state_dict)\n",
        "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "        # finetune model on support samples\n",
        "        for _ in range(inner_iter_num):\n",
        "            # supports_new = {}\n",
        "            # for k,v in supports.items():\n",
        "            #     supports_new[k] = v.to(device).squeeze(0)\n",
        "            for k in supports:\n",
        "                supports[k] = supports[k].to(model.device).squeeze(0)\n",
        "            outputs = model(**supports)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # evaluate fine-tuned model on query\n",
        "        model.eval()\n",
        "        outputs = model.generate(query[\"pixel_values\"].to(device))\n",
        "        cer = compute_cer(pred_ids=outputs, label_ids=query[\"labels\"])\n",
        "        valid_cer.append(cer)\n",
        "    return compute_confidence_interval(valid_cer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIsydOZQWbv",
        "outputId": "159c5b4f-047a-4c88-8e1f-11b3eb89538e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4.209980950708013"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_cer, best_std = fewshot_testing(model, fewshot_val_dataloader)\n",
        "best_cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8li8EFJe6RS",
        "outputId": "28bcf89a-664a-4734-a78a-38a5d7d90053"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:30<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 2.7390523053513642 += 0.915965384795954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_functorch/deprecated.py:100: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
            "  warn_deprecated('make_functional', 'torch.func.functional_call')\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_functorch/deprecated.py:65: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.grad is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.grad instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
            "  warn_deprecated('grad')\n",
            "loss 15.900981903076172: 100%|██████████| 2000/2000 [21:19<00:00,  1.56it/s]\n",
            "100%|██████████| 100/100 [01:30<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.6860801187690975 += 0.45653230124290495\n",
            "Best checkpoint found at epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 25.235246658325195: 100%|██████████| 2000/2000 [21:16<00:00,  1.57it/s]\n",
            "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.201715406162465 += 0.2698849438099793\n",
            "Best checkpoint found at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 10.685157775878906: 100%|██████████| 2000/2000 [21:18<00:00,  1.56it/s]\n",
            "100%|██████████| 100/100 [01:29<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.1310205866970948 += 0.10930752539283478\n",
            "Best checkpoint found at epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 14.53603744506836: 100%|██████████| 2000/2000 [21:18<00:00,  1.56it/s]\n",
            "100%|██████████| 100/100 [01:29<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0531909902496408 += 0.050901938522680545\n",
            "Best checkpoint found at epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 7.898475170135498: 100%|██████████| 2000/2000 [21:17<00:00,  1.57it/s]\n",
            "100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0767455950911833 += 0.1220590579449819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 10.606493949890137: 100%|██████████| 2000/2000 [21:17<00:00,  1.57it/s]\n",
            "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0242862797236993 += 0.024582452159994342\n",
            "Best checkpoint found at epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 12.865299224853516: 100%|██████████| 2000/2000 [21:17<00:00,  1.57it/s]\n",
            "100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.003720930232558 += 0.007256466518061305\n",
            "Best checkpoint found at epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 9.770898818969727: 100%|██████████| 2000/2000 [21:18<00:00,  1.56it/s]\n",
            "100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0399197860962566 += 0.06840283024053091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 7.15339469909668: 100%|██████████| 2000/2000 [21:18<00:00,  1.56it/s]\n",
            "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0125567435082141 += 0.02095103640624679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss 9.036985397338867: 100%|██████████| 2000/2000 [21:17<00:00,  1.57it/s]\n",
            "100%|██████████| 100/100 [01:27<00:00,  1.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cer: 1.0034166666666666 += 0.005195155531839253\n",
            "Best checkpoint found at epoch 9\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "num_epochs = 10\n",
        "inner_iter_num = 1\n",
        "valid_cer = []\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "best_cer, best_std = fewshot_testing(model, fewshot_val_dataloader)\n",
        "print(f\"cer: {best_cer} += {best_std}\")\n",
        "\n",
        "def compute_loss(params, **x):\n",
        "    output = func(params, **x)\n",
        "    return output.loss\n",
        "\n",
        "def tree_map(func, x, y):\n",
        "    print(isinstance(x[0], list))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(fewshot_train_dataloader)\n",
        "    for supports, query in pbar:\n",
        "        func, params = make_functional(model)\n",
        "        for k in supports:\n",
        "            supports[k] = supports[k].to(model.device).squeeze(0)\n",
        "            query[k] = query[k].to(model.device)\n",
        "\n",
        "        updated_params = params\n",
        "\n",
        "        for _ in range(inner_iter_num):\n",
        "            grad_weights = grad(compute_loss)(updated_params, **supports)\n",
        "            updated_params = [x - 5e-5 * y for x, y in zip(updated_params, grad_weights)]\n",
        "            updated_params = tuple(updated_params)\n",
        "\n",
        "        outputs = func(updated_params, **query)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        pbar.set_description(f\"loss {loss.item()}\")\n",
        "\n",
        "    cer, std = fewshot_testing(model, fewshot_val_dataloader)\n",
        "    print(f\"cer: {cer} += {std}\")\n",
        "    if cer < best_cer:\n",
        "        best_cer, best_std = cer, std\n",
        "        print(f\"Best checkpoint found at epoch {epoch}\")\n",
        "        torch.save(model.state_dict(), f'/content/best_epoch_{epoch}.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
